{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset.sequence import SkeletonClusteredSequence, ClusteredSequence\n",
    "import pickle as pkl\n",
    "import mxnet as mx\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "\n",
    "with open('clustered_sequences.pkl', 'rb') as file:\n",
    "    sequences = pkl.load(file)\n",
    "\n",
    "with open('skeleton_sequences.pkl', 'rb') as file:\n",
    "    skeletons = pkl.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "1692"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_skeleton_sequences = []\n",
    "for sk_seq in skeletons:\n",
    "    for seq in sequences:\n",
    "        if sk_seq.name_pattern == seq.name_pattern and sk_seq.start_frame == seq.gt['frame'].min():\n",
    "            clustered_skeleton_sequences.append(SkeletonClusteredSequence(seq, sk_seq))\n",
    "            break\n",
    "len(clustered_skeleton_sequences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from utils.missing_data import remove_ellipsis\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SinglePersonSequenceHandler:\n",
    "    def __init__(self, seq: SkeletonClusteredSequence):\n",
    "        self.seq = seq\n",
    "\n",
    "    def get_person_num(self):\n",
    "        return len(self.seq.person_features)\n",
    "\n",
    "    def get_person_ids(self):\n",
    "        return list(self.seq.person_features.keys())\n",
    "\n",
    "    def get_single_person_attributes(self, person_id):\n",
    "        array_index = self.get_person_ids().index(person_id)\n",
    "        klass = self.seq.cluster_labels[array_index]\n",
    "        skeleton_seq, confidence_seq, classes_seq, bboxes_seq, scores_seq = self._get_skeleton_seq(person_id)\n",
    "        return skeleton_seq, confidence_seq, classes_seq, bboxes_seq, scores_seq, klass\n",
    "\n",
    "    def get_single_person_as_train_record(self, person_id):\n",
    "        array_index = self.get_person_ids().index(person_id)\n",
    "        klass = self.seq.cluster_labels[array_index]\n",
    "        skeleton_seq = []\n",
    "        for skeleton_iter, person_ids in zip(self.seq.skeleton_seq.skeletons_seq, self.seq.skeleton_seq.person_ids):\n",
    "            if person_id not in person_ids:\n",
    "                skeleton_seq.append(...)\n",
    "                continue\n",
    "            person_idx = np.where(person_ids == float(person_id))[0]\n",
    "            skeleton_seq.append(skeleton_iter[person_idx])\n",
    "        return remove_ellipsis(skeleton_seq), klass\n",
    "\n",
    "    def _get_skeleton_seq(self, person_id):\n",
    "        skeleton_seq = []\n",
    "        confidence_seq = []\n",
    "        classes_seq = []\n",
    "        bboxes_seq = []\n",
    "        scores_seq = []\n",
    "        for skeleton_iter, confidence_iter, classes_iter, bboxes_iter, scores_iter, person_ids in zip(\n",
    "                self.seq.skeleton_seq.skeletons_seq,\n",
    "                self.seq.skeleton_seq.confidence_seq,\n",
    "                self.seq.skeleton_seq.classes_seq,\n",
    "                self.seq.skeleton_seq.bboxes_seq,\n",
    "                self.seq.skeleton_seq.scores_seq,\n",
    "                self.seq.skeleton_seq.person_ids):\n",
    "            if person_id not in person_ids:\n",
    "                skeleton_seq.append(...)\n",
    "                confidence_seq.append(...)\n",
    "                classes_seq.append(...)\n",
    "                bboxes_seq.append(...)\n",
    "                scores_seq.append(...)\n",
    "                continue\n",
    "            person_idx = np.where(person_ids == float(person_id))[0]\n",
    "            skeleton_seq.append(skeleton_iter[person_idx])\n",
    "            confidence_seq.append(confidence_iter[person_idx])\n",
    "            classes_seq.append(classes_iter[0][person_idx])\n",
    "            bboxes_seq.append(bboxes_iter[0][person_idx])\n",
    "            scores_seq.append(scores_iter[0][person_idx])\n",
    "        return remove_ellipsis(skeleton_seq), remove_ellipsis(confidence_seq), remove_ellipsis(\n",
    "            classes_seq), remove_ellipsis(bboxes_seq), remove_ellipsis(scores_seq)\n",
    "\n",
    "\n",
    "handler = SinglePersonSequenceHandler(clustered_skeleton_sequences[0])\n",
    "skeleton, confidence, classes, bboxes, scores, single_klass = handler.get_single_person_attributes(4)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 16:26:10.471036: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-08 16:26:11.201323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rafa/anaconda3/envs/sportstrack/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2023-01-08 16:26:11.201375: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rafa/anaconda3/envs/sportstrack/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2023-01-08 16:26:11.201380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19632, 32, 34)\n",
      "(19632,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 16:26:11.808539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:11.809505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:11.809586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:11.809871: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-08 16:26:11.810170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:11.810256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:11.810341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:12.198081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:12.198194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:12.198272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-08 16:26:12.198342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8699 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f726ca29200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f726ca29200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 16:26:15.844612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "(32, 256)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "with open('single_X.pkl', 'rb') as file:\n",
    "    X = np.array(pkl.load(file))\n",
    "\n",
    "with open('single_y.pkl', 'rb') as file:\n",
    "    y = np.array(pkl.load(file))\n",
    "\n",
    "X = X.reshape((X.shape[0], X.shape[1], X.shape[2] * X.shape[3]))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "hidden_state_ext = keras.models.load_model('hidden_state_ext.keras')\n",
    "# print(X[0][None,].shape)\n",
    "w1 = hidden_state_ext.predict(X[0][None,])\n",
    "w2 = hidden_state_ext.predict(X[1][None,])\n",
    "timesteps, hidden_shape = hidden_state_ext.layers[-1].output_shape[1:]\n",
    "timesteps, hidden_shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "pooler = Sequential()\n",
    "max_people_numb = 20\n",
    "pooler.add(tf.keras.layers.MaxPooling2D(pool_size=(1, max_people_numb), strides=(1, 1), padding='valid'))\n",
    "pooler.add(tf.keras.layers.Reshape((timesteps, hidden_shape)))\n",
    "# pooler.add(tf.keras.layers.Reshape((1,hidden_shape)))\n",
    "pooler.compile('adam', 'mean_squared_error')\n",
    "# print(w1)\n",
    "# print(w2)\n",
    "\n",
    "# inp =(np.array([w1.reshape(timesteps * hidden_shape,1), w2.reshape(timesteps * hidden_shape,1)]))\n",
    "# inp=(inp.T.reshape(1,timesteps * hidden_shape,max_people_numb , 1))\n",
    "#\n",
    "# w3 = pooler.predict(inp)\n",
    "# print(w3.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafa/anaconda3/envs/sportstrack/lib/python3.7/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.1` and `torch==1.8.1+cu111` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n",
      "  0%|          | 0/1692 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f723c7f2830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f723c7f2830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1692/1692 [16:46<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692 1692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "((1, 32, 256), 'football')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import gluoncv\n",
    "import cv2\n",
    "from utils.normalizers import skeleton_normalizer\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for seq in tqdm(clustered_skeleton_sequences):\n",
    "    handler = SinglePersonSequenceHandler(seq)\n",
    "    sequence_hidden_states = []\n",
    "    for person_id in handler.get_person_ids():\n",
    "        skeleton, single_klass = handler.get_single_person_as_train_record(person_id)\n",
    "        normalized_skeletons = np.squeeze(\n",
    "            np.array([np.reshape(skeleton_normalizer(single_skeleton), (1, 34)) for single_skeleton in skeleton]))\n",
    "        hidden_state = hidden_state_ext.predict(normalized_skeletons[None, :], verbose=0)\n",
    "        sequence_hidden_states.append(hidden_state)\n",
    "    first_20_people = sequence_hidden_states[:max_people_numb]\n",
    "    first_20_people += [[np.array([[-99999999] * hidden_shape]*timesteps)[None, :]] * (max_people_numb - len(first_20_people))][0]\n",
    "    shaped = (np.array([w.reshape(timesteps * hidden_shape, 1) for w in first_20_people]))\n",
    "    inp = (shaped.T.reshape(1, timesteps * hidden_shape, max_people_numb, 1))\n",
    "    pooled_over_people = pooler.predict(inp, verbose=0)\n",
    "    X.append(pooled_over_people)\n",
    "    y.append(seq.klass)\n",
    "\n",
    "    # first_100_people = hidden_states[:100]\n",
    "    # X.append(np.squeeze(np.array(normalized_skeletons)))\n",
    "    # y.append(single_klass)\n",
    "print(len(X), len(y))\n",
    "X[0].shape, y[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "with open('multi_X.pkl', 'wb') as file:\n",
    "    pkl.dump(X, file)\n",
    "\n",
    "with open('multi_y.pkl', 'wb') as file:\n",
    "    pkl.dump(y, file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
